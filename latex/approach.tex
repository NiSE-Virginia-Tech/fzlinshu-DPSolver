 \section{Approach}\label{sec:approach}
 
 \tool consists of three phases: DP problem recognition (Section~\ref{sec:recognize}), DP-oriented model description generation (Section~\ref{sec:generate}), and description selection (Section~\ref{sec:select}). 
 Phase II is taken only when Phase I identifies one or more solvable problems in a given MiniZinc model; and Phase III is taken only when Phase II generates multiple alternative models. 
 
 \subsection{Phase I: DP Problem Recognition}
 \label{sec:recognize}
If a problem can be efficiently solved with dynamic programming, it must have two  properties~\cite{T2012Introduction}:

\begin{enumerate}
    \item[P1.] \textbf{Optimal Substructures.} An optimal solution can be constructed from optimal solutions of its subproblems. This property ensures the usability of DP, because DP saves and uses only the optimal instead of all solutions to subproblems for optima calculation. 
    
   \item[P2.] \textbf{Overlapping Subproblems.} 
   When a problem is decomposed into subproblems, some  subproblems are repetitively solved. 
   This property ensures the usefulness of DP, because by memoizing solutions to subproblems, DP can eliminate repetitive computation. 
\end{enumerate}
Essentially, DP is applicable to a COP when optimal solutions to subproblems can be repetitively reused for optima calculation.
%non-optimal solutions to subproblems cannot lead to any optimal solution to the overall problem, and (2) different branches in a solution search tree overlap. DP optimizes constraint solving by pruning unpromising and redundant search paths. 
If a COP has both properties, we name it a 
\textbf{\emph{DP problem}}.  
 
 Given a COP modeled in MiniZinc, \tool recognizes a DP problem by taking three steps: 1) identifying 
 any array variable and accumulative function applied to those array elements, 
 2) converting constraints and objective functions to recursive functions of array elements (i.e., subproblems), and 
 3) checking recursive functions for the two properties. 

\paragraph*{Step 1: Candidate Identification}
\tool checks for any declared variable of the array data type because DP is usually applied to arrays. Additionally, if a variable is a set, as shown by the \codefont{knapsack} variable in Figure~\ref{fig:knapsack}, \tool converts it to a boolean array \codefont{b} such that ``\codefont{b[i]}'' indicates whether the $i^{th}$ item in the set is chosen or not. 
By doing so, \tool can also handle problems with set variables. We name the identified array or set variables \textbf{\emph{candidate variables}}. 
 
Next, \tool checks 
whether any candidate variable is used in at least one constraint and one objective function; if so, \tool may find optimization opportunities when enumerating all value assignments to the variable. 
In Figure~\ref{fig:knapsack}, both the constraint and objective can be treated as functions of the newly created array $b$ as below: 
\begin{equation}
\label{eq1}
   \sum_{i=1}^{N}b[i]*W[i] \le C \text{, where }b[i]\in\{0, 1\}  \tag{3.1}
\end{equation}
\begin{equation}
\label{eq2}
    maximize \sum_{i=1}^{N}b[i]*V[i] \text{, where }b[i]\in\{0, 1\}  \tag{3.2}
\end{equation}
When these functions have any accumulative operator (e.g., $sum$), it is feasible to further break down the problem into subproblems.
Thus, \tool treats the variable \codefont{b} together with related functions as a candidate for DP application.

\paragraph{Step 2: Function Conversion}
\tool tentatively converts relevant constraint and objective functions to step-wise recursive functions in order to identify subproblems and prepare for further property check. 
%Actually, this conversion procedure has two parts: from accumulative functions to recursive functions, and from recursive functions to DP-oriented recursive functions. 
%$(a)$ {$Accumulative\longrightarrow Recursive$.} 
Specifically, 
\tool unfolds all accumulative functions 
%to recursive functions and breaks down the given problem into simpler subproblems 
recursively. 
For instance, the constraint formula (\ref{eq1}) can be converted to
\begin{align}
\label{fun1}
f_0(W, b)&=0 \nonumber\\
f_1(W, b)&=f_0(W, b) + b[1] * W[1] \nonumber\\
c_1(W, b)&=C-f_1(W, b)\nonumber\\
c_1(W, b)&\ge 0 \nonumber\\
\ldots \tag{3.3}\\  
f_N(W, b)&=f_{N-1}(W, b) + b[N] * W[N] \nonumber\\
c_N(W, b)&=C-f_N(W, b) \nonumber\\
c_N(W, b)& \ge 0 \nonumber
\end{align}

Here, $f_i(W, b) (i\in[1, N])$ computes weight sums for the first $i$ items given (1) the item weight array $W$ and (2) the boolean array $b$.
The function $c_i(W, b)$ subtracts $f_i(W, b)$ from capacity $C$, to define the constraint for each subproblem. Here, $c_i$ means ``\emph{the remaining capacity limit for the last $(N-i)$ items}''. 
The value of $c_i$ helps decide whether an item should be added to the knapsack. Namely, if the weight of the $(i+1)^{th}$ item is greater than $c_i$'s value, the item should not be added.
%The value of $c_i$ is limited by the lower bound $(N - i) * min\{W[1], \ldots, W[N]\}$.
%, where $w$ is the minimum weight among all items. \tool scanned the input data to obtain $w > 0$, and simplified the lower bound to $0$. 
%The $c_i$ function essentially defines the constraint for each subproblem. 
%Here, $c_i$ means ``the remaining capacity limit for the last $(N-i)$ item''. 
%The value of $c_i$ helps decide whether an item can be added to the knapsack. Namely, if the weight of $(i+1)^{th}$ item is greater than $c_i$, this item cannot be added.
Actually, the $c_i$'s value is limited by the lower bound $min\{0, (N - i) * min\{W[1], \ldots, W[N]\}\}$. Because \tool scanned the input data and found all weight values to be greater than 0, the lower bound used in \ref{fun1} was simplified to 0.

Similarly, the objective function (\ref{eq2}) can be transferred to
\begin{align}
\label{fun2}
o_0(V, b)&=0 \nonumber\\
o_1(V, b)&=o_0(V, b) + b[1] * V[1]\nonumber\\
opt_1(V, b)&=max \text{ }o_1(V, b) \nonumber\\
\ldots \tag{3.4}\\
o_N(V, b)&=o_{N-1}(V, b) + b[N] * V[N]\nonumber\\
opt_N(V, b)&=max\text{ }o_N(V, b) \nonumber
%optimal[i][j]&=\left\{
%\right.
\end{align}

Here, $o_N(V, b)$ calculates the value summation.
The function $opt_i(V, b)$ defines the optimization goal for each subproblem related to the first $i$ items. When all possible value assignments are explored for $b$, $o_N(V, b)$ and $opt_N(V, b)$ functions can be executed to obtain the maximal summation. 

%$o1(V, b)$ can be used to produce the value summations this function produces the value summations accordingly to obtain the maximal summation. 

%records the value summations of some items. As with the original accumulative functions (Formula~\ref{eq1} and~\ref{eq2}),  these recursive functions still model the exhaustive search for subsets of N items. 


\paragraph{Step 3: Property Check}
With the converted functions, \tool checks for two properties in sequence. 

\paragraph{$(a)$ \emph{Verifying optimal substructures.}}
We first defined and proved the following theorem:

\begin{theorem}
\label{thm1}
Given two sets of functions, $O=\{o_0(\cdot), o_1(\cdot), o_2(\cdot),  \ldots, o_n(\cdot)\}$ and  $Opt=\{opt_1(\cdot), opt_2(\cdot), \ldots, opt_n(\cdot)\}$ (``$\cdot$'' is a placeholder for arguments), for any $i\in[1, n]$, suppose that
\begin{itemize}
    \item $o_i(\cdot)=h(o_{i-1}(\cdot), b[i])$ where $b$ is a candidate variable and h is monotonically increasing in $o_{i-1}(\cdot)$,  
    \item $opt_i(\cdot)=max\text{ }o_i(\cdot)$. 
    %, and \item $l(h(\cdot))=h(l(\cdot))$, namely, $l$ and $h$ are commutative.
\end{itemize}
Then $opt_i(\cdot)$ is monotonically increasing in $opt_{i-1}(\cdot)$. 
\end{theorem}

In this theorem, each $o_i(.)$ is a function, representing all possible value sums produced when $b[1..i]$ is assigned with different vector values, while $max\text{ }o_i(.)$ is a value, representing the maximum among those value sums.

\begin{proof}
For any $i\in[1, n]$, 
%since $o_{i-1}(\cdot) \le max \text{ }o_{i-1}(\cdot)$ and $h$ increases monotonically, we deduce that $h(o_{i-1}(\cdot))\le h(max \text{ }o_{i-1}(\cdot))$, namely, $o_i\le h(max \text{ }o_{i-1}(\cdot))$. As $h(max\text{ }o_{i-1}(\cdot))$ is the maximum value that $o_i(\cdot)$ can obtain, we have $max\text{ }o_i(\cdot)=h(max \text{ }o_{i-1}(\cdot))$.  
\begin{align}
    &\because o_{i-1}(\cdot) \le max \text{ }o_{i-1}(\cdot) \nonumber\\
    & \therefore h(o_{i-1}(\cdot), b[i])\le h(max \text{ }o_{i-1}(\cdot), b[i]) \nonumber\\
     &\therefore o_i(\cdot) \le h(max \text{ }o_{i-1}(\cdot), b[i])\nonumber\\ %(o_i(\cdot)=h(o_{i-1}(\cdot)))\nonumber\\
     &\therefore max\text{ }o_i(\cdot)=h(max \text{ }o_{i-1}(\cdot), b[i]), i.e.,  \nonumber \\
     & opt_i(\cdot)=h(opt_{i-1}(\cdot), b[i]) \nonumber
     %& \therefore l(max\text{ }o_i(\cdot))=l(h(max \text{ }o_{i-1}(\cdot)))\nonumber\\
     %&\therefore opt_i(\cdot)=l(h(max \text{ }o_{i-1}(\cdot)))\nonumber\\
     %&\therefore opt_i(\cdot)=h(l(max\text{ }o_{i-1}(\cdot))) (h \text{ and }l\text{ are commutative})\nonumber\\
    %&\therefore opt_i(\cdot)=h(opt_{i-1}(\cdot))\nonumber
\end{align}
Therefore, $opt_i(\cdot)$ monotonically increases in $opt_{i-1}(\cdot)$. The optimal solution can be composed with the optimal solution to a subproblem. \qedhere  
\end{proof}

Similarly, we defined and proved a related theorem when the $max$ function used in Theorem~\ref{thm1} is replaced with $min$. Furthermore, there are problems whose $opt_i(\cdot)$ functions are expressions of $max\text{ }o_i$ (e.g., $max\text{ }o_i+3$) instead of $max\text{ }o_i$ itself. 
To ensure the generalizability of our approach, we also consider such problems 
to have optimal substructures as long as $max\text{ }o_i$ is a function of $max\text{ }o_{i-1}$. 
This is because when the \textbf{\emph{extreme value}} ($max$ or $min$) related to a problem's optimal solution can be computed 
with the extreme values derived for subproblems,
we can always construct the optimal solution by reusing extreme values from subproblems. 

%we still generally consider such problems to have optimal substructures, because the optimal solution can be computed based on some maximum values computed for subproblems.  

Based on the above theorems, given converted objective functions, \tool locates the used $max$ or $min$ function and tentatively matches $h$. 
For each matched $h$ function, \tool takes the derivative to check for any monotonicity property.
%For each matched $l$, \tool checks whether the evaluation order between $l$ and $h$ can be swapped. 
For our example (function sets (\ref{fun2})), we have 
\begin{align}
    & h_{0/1}(o_{i-1}(\cdot))=o_{i-1}(\cdot)+b[i]*V[i]. \nonumber
   % & l_{0/1}(x)=x
\end{align}
Assuming $h_{0/1}$ to be continuous, \tool finds the derivative to be $\partial o_i(\cdot)/\partial o_{i-1}(\cdot)=1>0$. Therefore, $h_{0/1}$ increases monotonically in $o_{i-1}$. Solutions of the 0-1 knapsack problem have   optimal substructures.  
%Besides, \tool randomly samples multiple values of $b$ and compares the results of $l_{0/1}(h_{0/1}(\cdot)$ and $h_{0/1}(l_{0/1}(\cdot))$. Since these composed functions always produce the same values, \tool concludes that the two functions are commutative.  

%If a problem does not pass this property check, \tool stops its attempt to optimize constraint solving; otherwise, it continues with a second property check. 

\paragraph{$(b)$ \emph{Verifying overlapping subproblems.}} We defined and proved another theorem to facilitate property checking. 

\begin{theorem}
\label{thm2}
Given two sets of functions, $F=\{f_0(\cdot), f_1(\cdot), \ldots, f_n(\cdot)\}$ and $Con=\{c_0(\cdot), c_1(\cdot), \ldots, c_n(\cdot)\}$ (``$\cdot$'' is a placeholder for arguments), for 
any $i\in[1, n]$, suppose that 
\begin{itemize}
\item $f_0(\cdot)=v_0$ where $v_0$ is a constant, 
\item $f_i(\cdot)=p(f_{i-1}(\cdot), b[i])$ where $b$ is a variable, and
\item $c_i(\cdot)=q(f_{i-1}(\cdot), b[i])$. 
\end{itemize}
Then there exist overlapping subproblems of $f_n(\cdot)$ and $c_n(\cdot)$ 
between different value assignments of $b$.
\end{theorem}

\begin{proof}
This theorem includes two parts: 
(1) $f_n(\cdot)$ has overlapping subproblems; and (2)  $c_n(\cdot)$ has overlapping subproblems. Here we demonstrate the proof by induction for $f_n(\cdot)$. 
The proof for $c_n(\cdot)$ is similar. 

\begin{enumerate}
    \item \textbf{n=2:} For any two  assignments of $b$: $b_{A2}'$ and $b_{A2}''$, where $b_{A2}'\neq b_{A2}''$ but $b_{A2}'[1]=b_{A2}''[1]$, 
\begin{align}
    & \because \text{The first elements of both arrays are identical,}\nonumber\\
    & \therefore \text{The evaluation procedures of }
    f_1(\cdot)=p(f_0(\cdot), b[1]) \nonumber\\
    &\text{remains the same given $b_{A2}'$ and $b_{A2}''$. } \nonumber\\
    & \therefore \text{When $f_2(\cdot)$ is computed based on $f_1(\cdot)$, the } \nonumber\\
    & \text{calculation procedure of $f_1(\cdot)$ overlaps between $b_{A2}'$ } \nonumber\\
    &\text{and $b_{A2}''$. Thus, $f_n(\cdot)$ has overlapping subproblems.} \nonumber
\end{align}
    \item \textbf{n=k (k$>$2):} Assume the theorem to hold. Namely, there exist two assignments $b_{Ak}'$ and $b_{Ak}''$ ($b_{Ak}'\neq b_{Ak}''$), between which the evaluation procedures of  $f_n(\cdot)$ have overlapping subproblems. 
    \item \textbf{n=k+1:} To prove the theorem, we compose two assignments: $b_{A(k+1)}'$ and $b_{A(k+1)}''$, such that $b_{A(k+1)}'[1:k]=b_{Ak}'$ and $b_{A(k+1)}''[1:k]=b_{Ak}''$. %$b_{A(k+1)}'[k+1]=b_{A(k+1)}''[k+1]$. 
    Intuitively, these arrays separately include $b_{Ak}'$ and $b_{Ak}''$ as the first $k$ elements.  
\begin{align}
    & \because f_{k+1}(\cdot)=p(f_{k}(\cdot), b[k+1]) \nonumber \\
    %& \because %c_{k+1}(\cdot)=q(f_{k}(\cdot), b[k+1])\nonumber\\
   & \therefore \text{The function depends on the evaluation result of $f_{k}(\cdot)$,}\nonumber\\
   & \text{a function computed based on the first $k$ elements} \nonumber \\
   & \because \text{ The evaluation processes of $f_k(\cdot)$ between $b_{Ak}'$ and $b_{Ak}''$} \nonumber \\
   & \text{have overlapping subproblems} \nonumber \\
   & \therefore \text{The evaluation processes of $f_{k+1}(\cdot)$ between $b_{A(k+1)'}$}\nonumber \\
   &\text{ and $b_{A(k+1)}''$ also overlap.} \nonumber 
\end{align}    
\end{enumerate}
Therefore, when different value assignments of $b$ are explored, there are overlapping subproblems to resolve. 
\end{proof}

Based on Theorem~\ref{thm2}, given converted constraint functions, \tool tries to match $p$ and $q$ by using or unfolding the formulas of 
 $f$ and $c$ functions. Thus, for our example (function sets (\ref{fun1})), the matched functions are
\begin{align}
    & p_{0/1}(f_{i-1}(\cdot), b[i])=f_{i-1}(\cdot)+b[i]*W[i], \nonumber\\
    & q_{0/1}(f_{i-1}(\cdot), b[i])=C-f_{i-1}(\cdot)-b[i]*W[i]. \nonumber
   % & l_{0/1}(x)=x
\end{align}
Notice that $p_{0/1}$ is derived from the $f$ formulas, while $q_{0/1}$ is obtained when \tool replaces the occurrence of $f$ in $c$ formulas. 
 With such matched functions found, the 0-1 knapsack problem passes the property check. 
 
DP trades space for time by storing and reusing optimal solutions to subproblems. 
If a problem has the first property only, \tool does not proceed to Phase II. Because there is no reuse of intermediate results, the extra space consumption for caching those results will not bring any execution speedup. 
However, if a problem has the second property only, even though DP is not applicable, \tool still creates a table to memoize \emph{all} intermediate results  for data reuse and runtime overhead reduction. 

  
 
%When a problem passes both property checks, \tool determines the problem to be a DP problem. If a problem only has the first property, even though DP is applicable, we do not apply it because 
 
 \subsection{Phase II: DP-Oriented Model Generation}
 \label{sec:generate}
 
%To efficiently solve a DP problem with a general-purpose constraint solver, \tool rewrites  
The above-mentioned recursive functions (e.g., function sets (\ref{fun1}) and (\ref{fun2})) are not directly usable by DP for efficient search because no redundant computation is eliminated. 
%of redundant computation. For instance, given two value assignments of $b$ (i.e., $b_N'$ and $b_N''$) that have identical values for the first $(N-1)$ items,
%$f_{N-1}(W, b)$ repeats the same computation procedure and outputs the same value. To eliminate redundant computation, 
We need to rewrite those functions 
such that intermediate results for subproblems are stored to a table and are used to replace repetitive computation. 
%The reason is that a DP algorithm usually leverages a table to record solutions for subproblems and reuse these solutions to handle bigger subproblems. 

Specifically, given (1) a candidate variable $b$, (2) a series of constraint functions $c(\cdot)$, (3) a series of objective functions $o(\cdot)$, and (4) the extreme value we care about (i.e., $max$ or $min$), \tool creates a two-dimension array $M$ such that 
\begin{align}
    M[i, c_i]&=extreme\text{ }o_i(\cdot) \nonumber\\
           &=extreme\text{ }h(extreme\text{ }o_{i-1}(\cdot), b[i]) \nonumber \\
           &=extreme\text{ }h(M[i-1, c_{i-1}], b[i]) \tag{3.5}
\end{align}
In $M$, $i$ corresponds to the array index range of $b$,
$c_i$ represents the corresponding valid constraint value, while the cell $M[i, c_i]$ saves the extreme value computed for each subproblem, meaning ``\emph{given the first i elements and the constraint value $c[i]$, what is the extreme value of $o_i$}?''. When different values of $b[i]$ are enumerated, the corresponding $c_{i-1}$ can be different. Thus, multiple cells in the row $M[i-1]$ may be reused for computation, and $M[i, c_i]$ is usually decided
by the value comparison between these cells. 
By generating such table-driven recursive functions to model a DP problem, \tool can produce a DP-oriented MiniZinc description. 

Figure~\ref{fig:knapsack2} shows the newly generated model for our example. In particular, \codefont{solve::seq\_search} is used so that the solver does not apply any built-in basic optimization when solving the problem. 
The space complexity of creating the $dpvalue$ table is $O(NC)$, while the time complexity is $O(NC)$. This time complexity refers to the whole resolution process because the DP-oriented model is resolved by propagation only.
%\todo{1. Is this MiniZinc model? 2. More questions concerning the nested 0-1 knapsack problem (space/time complexity). 3. Without optimal substructure, is there any optimization? 4. Problems with after effect. How are the results saved?} 

	\begin{figure}[htb]
\begin{lstlisting}[frame=single]
% Input arguments
    ... % unchanged N, C, V, W

% Variables
    array[0..N, 0..C] of var int: dpvalue;

% Constraints
    constraint
      forall(j in 0..C) (
        dpvalue[0,j] = 0
      );
      function var int: calcValue(var int: i, 
        var int: j, var int: k) =
        if j-W[i]*k>=0 then
          dpvalue[i-1,j-W[i]*k]+V[i]*k
        else
          0
        endif;
    constraint
      forall(i in 1..N) (
        forall (j in 0..C) (
          dpvalue[i,j]=max(kk in 0..1)(calcValue(i,j,kk))
        )
      );

% Objective function    
    solve::seq_search([int_search(dpvalue,
      input_order,indomain_split,complete)]) 
      maximize max(j in 0..C)(dpvalue[N,j]);
\end{lstlisting}
\caption{DP-oriented model of the 0-1 knapsack problem}\label{fig:knapsack2}
\end{figure}

\textbf{Handling of Side Constraints.} 
Some COP models are defined with \emph{side constraints} in addition to accumulative functions. \tool can still process such models by converting each side constraint to an \codefont{if}-clause. 
For instance, a variant of the 0-1 knapsack problem can have an additional requirement as ``\emph{Item 3 must be put in the knapsack}'', which can be expressed as a side constraint ``\codefont{3 in knapsack}''. For this side constraint, \tool generates an \codefont{if}-clause ``\codefont{if i==3 then if k!=1 then 0 else ...}'' and inserts it to the beginning of the \codefont{calcValue(...)} function in Figure \ref{fig:knapsack2}.

%In addition, for other constraints without accumulative functions, \tool simply converts each constraint to an ``if-clause'', and then inserts it to the ``calcValue(...)'' function. For example, if we add a side constraint ``\texttt{3 in knapsack}'' to the 0-1 knapsack model, it will be converted to ``\texttt{if i==3 then if k!=1 then 0 else ...}'' and inserted to the beginning of the ``calcValue(...)'' function in Figure \ref{fig:knapsack2}.
 
 
 \subsection{Phase III: DP-Oriented Model Selection}
 \label{sec:select}
 
 For some COP problems, \tool can create multiple alternative optimization strategies. Given the space limit (e.g., 4GB) specified by users when they run the tool via commands, \tool automatically estimates the time/space complexity of each alternative, chooses the models whose table sizes are within the space limit, and then recommends the model with the maximum speedup.  
 
To better explain this phase, we take the nested 0-1 knapsack problem as another exemplar DP problem (see Figure~\ref{fig:knapsack3}).  
 	\begin{figure}[htb]
\begin{lstlisting}[frame=single]
% Input arguments
    int: N1, N2, C1;
    array[1..N1] of int: V1, W1;
    array[1..N2] of int: V2, W2;

% Variables
    var int C2;
    var set of 1..N1: K1;
    var set of 1..N2: K2;

% Constraints
    constraint sum (i in K1)(W1[i]) <= C1;
    constraint C2 = sum (i in K1)(V1[i]);
    constraint sum (i in K2)(W2[i]) <= C2;
    
% Objective function
    solve maximize sum (i in K2)(V2[i]);

output ["\(K2)"];
\end{lstlisting}
\caption{Model of the nested 0-1 knapsack problem. There are two sets of items ($N1$ and $N2$).
Choose items in both sets to separately fill two knapsacks: $K1$ and $K2$. If the value sum in $K1$ is used as the capacity of $K2$, try to maximize the value sum in $K2$.}\label{fig:knapsack3}
\end{figure}
When analyzing the problem, \tool identifies two candidate variables---$b_1$ and $b_2$---to separately represent subsets of $N1$ and $N2$.
%---the boolean array reflecting different subsets of $N1$, and $b2$---another boolean array corresponding to subsets of $N2$. 
Since both variables are related to constraints, \tool explores and assesses three potential ways to create DP-oriented models: 

 \begin{enumerate}
     \item[(a)] Optimization based on $b_2$: %We use $C2$ to record the capacity of $K2$ (i.e., the maximum value sum in $K1$). 
     When exhaustive search is leveraged to
     fill $K1$ in various ways, for each produced $C2$ value, the optimization problem becomes a regular 0-1 knapsack problem---a DP problem. Therefore, \tool can create a DP-oriented model, with both time and space complexity as $O(N2C2)$. Since the exhaustive search handles such 0-1 knapsack problems with $O(2^{N2})$ time complexity, the optimization speedup is estimated as $(2^{N2}/N2C2)$.
     
      \item[(b)] Optimization based on $b_1$: When exhaustive search is used to fill $K2$ in various ways, for each generated weight sum $sum_w$ in $K2$, we need the value sum in $K1$ (i.e., C2) to be no less than $sum_w$ while the weight sum in $K1$ to be no more than $C1$. Because these converted problems do not have any objective function, they are not considered as DP problems. \tool does not create any model for optimization. 
     
     \item[(c)] Optimization based on $b_1$ and $b_2$: \tool concatenates $b_1$ and $b_2$ to create a larger array $b_3$. In $b_3$, the first $N1$ elements are involved in the first two constraints, while the last $N2$ elements are related to the third constraint and objective function. Thus, $b_3$ is still a candidate variable and the given problem is a DP problem. The exhaustive search obtains $O(2^{N1}2^{N2})$ time complexity. In comparison, DP search has both time and space complexity as 
     $O((N1+N2)C1Sum)$ time complexity, where $Sum$ is the upper bound of $sum_w$. Consequently, the estimated speedup is $(2^{N1}2^{N2}/(N1+N2)C1Sum)$. 
 \end{enumerate}
 
 	\begin{figure}[htb]
\begin{lstlisting}[frame=single]
% Input arguments
    ... % unchanged N1, N2, C1, V1, W1, V2, W2
    int: maxW1 = 100;
    % the maximum value of W1 obtained from the input

% Variables
    var int: C2;
    array[0..N1+N2,0..C1,0..maxW1*N1] of var int: dpvalue;

% Constraints
    constraint
      forall(j in 0..C1) (
        forall(k in 0..maxW1*N1)
          dpvalue[0,j,k] = 0
      );
      function var int: calcValue(var int: i, 
        var int: j, var int: k, var int: l) =
        if i <= N1 then
          if j-W1[i]*l>=0 then
            if i == N1
              /\ k!=dpvalue[i-1,j-W1[i]*l,k]+V1[i]*l then
              0
            else
              dpvalue[i-1,j-W1[i]*l,k]+V1[i]*l
            endif  
          else
            0
          endif
        else
          if k-W2[i-N1]*l>=0 then
            dpvalue[i-1,j,k-W2[i-N1]*l]+V2[i-N1]*l
          else
            0
          endif
        endif;
    constraint
      forall(i in 1..N) (
        forall (j in 0..C1) (
          forall (k in 0..maxW1*N1)
            dpvalue[i,j,k]
              =max(ll in 0..1)(calcValue(i,j,k,ll))
        )
      );

% Objective function    
    solve::seq_search([int_search(dpvalue,
      input_order,indomain_split,complete)]) 
      maximize max(j in 0..C1)
      (max(k in 0..maxW1*N1)(dpvalue[N,j,k]));
\end{lstlisting}
\caption{DP-oriented model of the nested 0-1 knapsack problem when
the optimization is based on both $b1$ and $b2$}\label{fig:knapsack4}
\end{figure}

 When the two separately generated models described in (a) and (c) both have their tables smaller than the space limit, \tool suggests the one with more speedup (i.e., (c)).
  Figure \ref{fig:knapsack4} shows the DP-oriented model resulted from (c).
  Intuitively, the more array variables are involved in optimization (e.g., $(b1, b2)$ vs.~$b2$), the more performance gain \tool is likely to obtain.  